## Argument for the NN Model I chose.

After much experimentation and trial-and-error, I found my most accurate model by using one convolutional, pooling and flattening layer, followed by two hidden layers with dropout and an output layer. Every input image has a dimension of 30x30, so 900 data points (without considering the differences in RGB color). Though it is possible to skip convolution, I decided to use one layer of it to apply 43 filters to my data. Then, I simplified down the number of data points I was using for each image through pooling (and flattening, to fit the shape for neural network use). I experimented with different numbers of hidden layers in my neural network, with different dropout rates and sizes. I received diminishing marginal returns on for accuracy score as I increased size greater and greater (ex: 500). I received extremely low testing accuracies (ex: 0.05) when my dropout was too high (ex: 0.5). However, hidden layers in general helped my model the more I added.

In the end, I chose a dropout rate of 0.1 to help my model avoid overfitting. I stuck with two moderately sized (256 units and 128 units) hidden layers and an output layer with 43 units to represent my 43 categories of street signs. Before running my data through these neural network layers, I kept one iteration of convolution/pooling/flattening in order to simplify my model from the outset. Overall, I was pleased to score around 94-95% accuracy on my testing data.